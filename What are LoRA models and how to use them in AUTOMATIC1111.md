LoRA模型是小型的稳定扩散模型，可以对标准检查点模型进行微小的更改。它们通常比检查点模型小10到100倍。这使得它们对那些拥有大量模型收藏的人非常有吸引力。
这是一个针对以前没有使用过LoRA模型的初学者的教程。您将学习什么是LoRA模型，在哪里可以找到它们，以及如何在AUTOMATIC1111 GUI中使用它们。然后，您将在最后找到LoRA模型的一些演示。

# What are LoRA models?

LoRA (Low-Rank Adaptation)是一种用于稳定扩散模型微调的训练技术。但我们已经有了训练技术，如Dreambooth和文本反转。LoRA有什么大不了的?LoRA在文件大小和训练能力之间提供了很好的平衡。Dreambooth功能强大，但会产生较大的模型文件(2-7 gb)。文本倒转是很小的(大约100 kb)，但是你不能做那么多。
LoRA介于两者之间:它们的文件大小是可管理的(2 - 200 mb)，而且训练能力也不错。喜欢试验模型的Stable Diffusion用户可以告诉你他们的本地存储是多么快地被填满。由于数量庞大，用个人电脑很难维持收藏。LoRA是存储问题的出色解决方案。

与文本反转一样，您不能单独使用LoRA模型。它必须与模型检查点文件一起使用。LoRA通过对附带的模型文件应用小的更改来修改样式。

# How does LoRA work?

LoRA对稳定扩散模型中最关键的部分:交叉注意层进行了微小的改变。它是模型中图像和提示符相遇的部分。研究人员发现，对模型的这一部分进行微调就足以实现良好的训练。
交叉注意层是下面稳定扩散模型架构中的黄色部分。

![](https://i0.wp.com/stable-diffusion-art.com/wp-content/uploads/2023/02/image-183.png?w=800&ssl=1)

交叉注意层的权重以矩阵的形式排列。矩阵就是排列在列和行中的一堆数字，就像Excel电子表格一样。LoRA模型通过向这些矩阵添加权重来微调模型。
如果LoRA模型文件需要存储相同数量的权重，它们如何才能更小?LoRA的诀窍是将一个矩阵分解成两个更小的(低秩)矩阵。这样它可以存储更少的数字。让我们用下面的例子来说明这一点。
假设模型有一个1000行2000列的矩阵。在模型文件中需要存储2,000,000个数字(1,000 x 2,000)。LoRA将矩阵分解为1000 × 2矩阵和2 × 2000矩阵。只有6000个数字(1000 × 2 + 2 × 2000)，少了333倍。这就是为什么LoRA文件要小得多。
